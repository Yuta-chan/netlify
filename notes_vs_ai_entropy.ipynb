{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d74a5f",
   "metadata": {},
   "source": [
    "# Information Theory and Website Relationship  \n",
    "### Shannon Entropy, Conditional Entropy, Mutual Information, KL Divergence  \n",
    "\n",
    "This notebook explores how fundamental concepts of Information Theory can be applied to compare the summaries generated by an AI system (e.g., NotebookLM and Gemini) with my own notes, published on a simple website.  \n",
    "\n",
    "The analysis connects with the contents of the course *Introduction to Artificial Intelligence* taught by **Professor Keita Tokuda** at **Juntendo University**, relating Shannon Entropy, Conditional Entropy, Mutual Information, and KL Divergence to the process of summarization.  \n",
    "\n",
    "**Author:** Judith Urbina  \n",
    "**Project:** Final Report – Website for Comparing Human Notes and AI Summaries  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63cd57e",
   "metadata": {},
   "source": [
    "## Token Vocabulary\n",
    "\n",
    "Imagine a vocabulary of **6 tokens**. Both my notes and the AI-generated summary have been tokenized using this set of 6 tokens.  \n",
    "This allows us to analyze the informational relationship between both texts using concepts from information theory, such as **Shannon entropy**, **conditional entropy**, **mutual information**, and **KL divergence**.\n",
    "\n",
    "---\n",
    "\n",
    "### What does each token represent?\n",
    "\n",
    "Each token ($t_1$ to $t_6$) is a basic unit of information that appears in both texts.  \n",
    "From the joint and marginal distributions of these tokens, we can calculate how much information my notes and the AI summary share, as well as how much uncertainty remains when knowing one or the other.\n",
    "\n",
    "---\n",
    "\n",
    "> **Example tokens:**  \n",
    "> - $t_1$: \"information\"  \n",
    "> - $t_2$: \"theory\"  \n",
    "> - $t_3$: \"web\"  \n",
    "> - $t_4$: \"entropy\"  \n",
    "> - $t_5$: \"mutual\"  \n",
    "> - $t_6$: \"divergence\"\n",
    "\n",
    "---\n",
    "\n",
    "Next, we explore how these tokens relate and what they reveal about the similarity and difference between my notes and the AI summary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8ccaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t1', 't2', 't3', 't4', 't5', 't6']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens\n",
    "[f\"t{i}\" for i in range(1, 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b2004",
   "metadata": {},
   "source": [
    "### Joint Distribution $p(x, y)$\n",
    "\n",
    "The **joint distribution** $p(x, y)$ describes the probability of each token pair, where:\n",
    "- $X$ represents my tokenized notes\n",
    "- $Y$ represents the tokenized AI summary\n",
    "\n",
    "This matrix captures how often each token from my notes appears together with each token from the AI summary.  \n",
    "The sum of all entries in $p(x, y)$ must be **1**, ensuring it is a valid probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "|        | $t_1$ | $t_2$ | $t_3$ | $t_4$ | $t_5$ | $t_6$ |\n",
    "|--------|-------|-------|-------|-------|-------|-------|\n",
    "| **$t_1$** | 0.28  | 0.04  | 0     | 0     | 0     | 0     |\n",
    "| **$t_2$** | 0.02  | 0.19  | 0     | 0     | 0     | 0     |\n",
    "| **$t_3$** | 0     | 0     | 0.15  | 0.03  | 0     | 0     |\n",
    "| **$t_4$** | 0     | 0     | 0.02  | 0.10  | 0     | 0     |\n",
    "| **$t_5$** | 0     | 0     | 0     | 0     | 0.08  | 0.02  |\n",
    "| **$t_6$** | 0     | 0     | 0     | 0     | 0.01  | 0.06  |\n",
    "\n",
    "---\n",
    "\n",
    "This table visually summarizes the relationship between the tokens in both texts, forming the basis for further information-theoretic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "092c5c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.04, 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.02, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.03, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.02, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.02],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.01, 0.  ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pxy = np.zeros((6,6), dtype=float)\n",
    "other_entries = {\n",
    "    (0,1):0.04,\n",
    "    (1,0):0.02,\n",
    "    (2,3):0.03,\n",
    "    (3,2):0.02,\n",
    "    (4,5):0.02,\n",
    "    (5,4):0.01\n",
    "}\n",
    "for (i,j), v in other_entries.items():\n",
    "    pxy[i,j]=v\n",
    "pxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22afed4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28, 0.04, 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.02, 0.19, 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.15, 0.03, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.02, 0.1 , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.08, 0.02],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.01, 0.06]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fill_diagonal(pxy,[0.28, 0.19, 0.15, 0.10, 0.08, 0.06])\n",
    "pxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba586d50",
   "metadata": {},
   "source": [
    "Ensure the sum of the joint probabilities is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59b8eeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234fbbc",
   "metadata": {},
   "source": [
    "## Marginal Probabilities of the Tokens\n",
    "\n",
    "The marginal probability distributions for each token are calculated as follows:\n",
    "\n",
    "- **For X (my notes):**  \n",
    "    Each entry in `px` represents the probability of a token appearing in my notes.\n",
    "\n",
    "- **For Y (AI summary):**  \n",
    "    Each entry in `py` represents the probability of a token appearing in the AI summary.\n",
    "\n",
    "| Token | $p_X(x)$ | $p_Y(y)$ |\n",
    "|-------|----------|----------|\n",
    "| t1    | 0.29     | 0.27     |\n",
    "| t2    | 0.22     | 0.24     |\n",
    "| t3    | 0.18     | 0.17     |\n",
    "| t4    | 0.12     | 0.13     |\n",
    "| t5    | 0.07     | 0.06     |\n",
    "| t6    | 0.06     | 0.07     |\n",
    "\n",
    "These probabilities summarize how frequently each token appears in the respective distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf21c6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.29, 0.22, 0.18, 0.12, 0.07, 0.06]),\n",
       " array([0.27, 0.24, 0.17, 0.13, 0.06, 0.07]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px = pxy.sum(axis=1)\n",
    "py = pxy.sum(axis=0)\n",
    "px,py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffbab1",
   "metadata": {},
   "source": [
    "### Entropies and Conditional Entropies\n",
    "\n",
    "The **conditional entropy** $H(X|Y)$ quantifies the remaining uncertainty in my notes ($X$) after knowing the AI summary ($Y$).  \n",
    "A lower value of $H(X|Y)$ means the summary captures more information from my notes, leaving less uncertainty.\n",
    "\n",
    "#### Calculated Values\n",
    "\n",
    "- **Entropy of my notes ($H(X)$):**  \n",
    "    $H(X) = 2.32$ bits\n",
    "\n",
    "- **Entropy of the AI summary ($H(Y)$):**  \n",
    "    $H(Y) = 2.33$ bits\n",
    "\n",
    "- **Joint entropy ($H(X, Y)$):**  \n",
    "    $H(X, Y) = 2.99$ bits\n",
    "\n",
    "- **Conditional entropy ($H(X|Y)$):**  \n",
    "    $H(X|Y) = 0.66$ bits\n",
    "\n",
    "- **Conditional entropy ($H(Y|X)$):**  \n",
    "    $H(Y|X) = 0.67$ bits\n",
    "\n",
    "---\n",
    "\n",
    "These values show how much information is shared and how much uncertainty remains between my notes and the AI summary.  \n",
    "The **conditional entropies** are much lower than the individual entropies, indicating a strong informational relationship between both texts.\n",
    "\n",
    "The **maximum value of the conditional entropy** is $\\log_2(6) \\approx 2.58$ bits, since the vocabulary length is 6.  \n",
    "The entropies of my notes and the AI summary are relatively high, while the joint entropy is intermediate and the conditional entropies are low.  \n",
    "This means that, although both texts are not strictly dependent on each other, they share a significant amount of context—knowing one leaves little uncertainty about the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c8989ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.169925001442312"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(len(pxy.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc885e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(len(px.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "517bf4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.322940778767583, 2.3334757516728257, 2.989817560149396)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log2\n",
    "Hx = -sum(pxi*log2(pxi) for pxi in px if pxi > 0)\n",
    "Hy = -sum(pyi*log2(pyi) for pyi in py if pyi > 0)\n",
    "Hxy = -sum(pxyi*log2(pxyi) for pxyi in pxy.flatten() if pxyi > 0)\n",
    "Hx,Hy,Hxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63596a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6563418084765704, 0.6668767813818133)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hx_given_y = Hxy - Hy\n",
    "Hy_given_x = Hxy - Hx\n",
    "Hx_given_y, Hy_given_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76aa289",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "**Mutual information** quantifies the degree of shared information between my notes and the AI summary.  \n",
    "It measures how much knowing one text reduces uncertainty about the other.  \n",
    "A higher mutual information value indicates greater overlap and similarity in the information content.  \n",
    "The maximum possible mutual information is the minimum of the entropy of my notes and the entropy of the AI summary.\n",
    "\n",
    "Mutual information is computed using the entropies of the individual distributions and their joint distribution:\n",
    "\n",
    "$$\n",
    "I(X; Y) = H(X) + H(Y) - H(X, Y)\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n",
    "### KL Divergence\n",
    "\n",
    "**KL divergence** measures how different the distribution of tokens in my notes is from the distribution in the AI summary.  \n",
    "It tells us how much extra information is needed from the AI summary to describe my notes.  \n",
    "Unlike mutual information, KL divergence does not have a fixed maximum; the closer it is to zero, the more similar the distributions are.\n",
    "\n",
    "KL divergence is calculated using the probabilities of each token in both distributions:\n",
    "\n",
    "$$\n",
    "D_{KL}(P_X \\| P_Y) = \\sum_{i} p_X(i) \\log_2 \\frac{p_X(i)}{p_Y(i)}\n",
    "$$\n",
    "\n",
    "- **Mutual Information:**  \n",
    "    $I(X; Y) = 1.67$ bits\n",
    "\n",
    "- **KL divergence:**  \n",
    "    $D_{KL}(P_X \\| P_Y) = 0.0055$ bits\n",
    "\n",
    "A lower mutual information means the content in my notes and in the AI summary are very different.  \n",
    "A lower KL divergence means the distributions are very similar; a higher value means they are more different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4b75473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.322940778767583, 2.3334757516728257, 2.322940778767583)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hx, Hy, min(Hx,Hy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9170f130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.666598970291012"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ixy = Hx + Hy - Hxy\n",
    "Ixy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0b5881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005490165858668312"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL = np.sum(px * np.log2(px / py))\n",
    "KL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
